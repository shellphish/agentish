# ==========================================
# MODEL INITIALIZATION AND LANGFUSE SETUP
# ==========================================

def _load_env_config():
    """Load environment configuration for LLM and Langfuse from model_config.yaml or environment variables."""
    import os

    # Try to load from model_config.yaml if MODEL_CONFIG_PATH is set
    config_path = os.getenv("MODEL_CONFIG_PATH")
    if config_path and os.path.exists(config_path):
        try:
            import yaml
            with open(config_path, 'r') as f:
                yaml_config = yaml.safe_load(f)

            # Extract provider configuration from top-level keys
            provider_type = yaml_config.get('provider_type', 'openai')

            # Extract provider-specific config
            result = {
                "provider_type": provider_type,
                "llm_model_name": yaml_config.get('model', 'gpt-4'),
                "llm_temperature": float(yaml_config.get('temperature', 0.0)),
                "llm_context_window": int(yaml_config.get('context_window', 8192)),
                "llm_max_output_tokens": int(yaml_config.get('max_output_tokens', 4096)),
            }

            # Provider-specific settings
            if provider_type == 'llamacpp':
                llamacpp = yaml_config.get('llamacpp', {}) or {}
                result["llamacpp_endpoint"] = llamacpp.get('endpoint', '')
                result["llamacpp_api_key"] = llamacpp.get('api_key', '')
            elif provider_type == 'litellm':
                litellm = yaml_config.get('litellm', {}) or {}
                result["litellm_endpoint"] = litellm.get('endpoint', '')
                result["litellm_api_key"] = litellm.get('api_key', '')
            elif provider_type == 'openai':
                openai_config = yaml_config.get('openai', {}) or {}
                result["openai_api_key"] = openai_config.get('api_key', '')
                result["openai_endpoint"] = openai_config.get('endpoint', '')

            # Langfuse configuration
            langfuse = yaml_config.get('langfuse', {}) or {}
            result["langfuse_public_key"] = langfuse.get('public_key', '')
            result["langfuse_secret_key"] = langfuse.get('secret_key', '')
            result["langfuse_host"] = langfuse.get('host', 'https://cloud.langfuse.com')

            # Check if tracing is enabled
            result["enable_tracing"] = yaml_config.get('enable_tracing', False)

            return result
        except Exception as e:
            print(f"Warning: Failed to load config from {config_path}: {e}")
            print("Falling back to environment variables")

    # Fallback to environment variables
    return {
        "provider_type": os.getenv("PROVIDER_TYPE", "openai"),
        "llm_model_name": os.getenv("LLM_MODEL_NAME", "gpt-4"),
        "llm_temperature": float(os.getenv("LLM_TEMPERATURE", "0.0")),
        "llm_context_window": int(os.getenv("LLM_CONTEXT_WINDOW", "8192")),
        "llm_max_output_tokens": int(os.getenv("LLM_MAX_OUTPUT_TOKENS", "4096")),

        # Provider-specific settings
        "llamacpp_endpoint": os.getenv("LLAMACPP_ENDPOINT", ""),
        "llamacpp_api_key": os.getenv("LLAMACPP_API_KEY", ""),
        "litellm_endpoint": os.getenv("LITELLM_ENDPOINT", ""),
        "litellm_api_key": os.getenv("LITELLM_API_KEY", ""),
        "openai_api_key": os.getenv("OPENAI_API_KEY", ""),
        "openai_endpoint": os.getenv("OPENAI_ENDPOINT", ""),

        # Langfuse configuration
        "langfuse_public_key": os.getenv("LANGFUSE_PUBLIC_KEY", ""),
        "langfuse_secret_key": os.getenv("LANGFUSE_SECRET_KEY", ""),
        "langfuse_host": os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com"),

        # Tracing flag
        "enable_tracing": os.getenv("ENABLE_TRACING", "false").lower() == "true",
    }

ENV_CONFIG = _load_env_config()


def _create_langfuse_handler():
    """Create Langfuse callback handler if configured and enabled."""
    # Check if tracing is enabled
    if not ENV_CONFIG.get("enable_tracing", False):
        return None

    if not ENV_CONFIG.get("langfuse_public_key") or not ENV_CONFIG.get("langfuse_secret_key"):
        return None

    try:
        import os
        os.environ["LANGFUSE_PUBLIC_KEY"] = ENV_CONFIG["langfuse_public_key"]
        os.environ["LANGFUSE_SECRET_KEY"] = ENV_CONFIG["langfuse_secret_key"]
        os.environ["LANGFUSE_HOST"] = ENV_CONFIG["langfuse_host"]

        # CallbackHandler reads from environment variables automatically
        return CallbackHandler()
    except Exception as e:
        print(f"Warning: Failed to initialize Langfuse: {e}")
        return None


def init_chat_model(model_name: str = None, temperature: float = None, max_tokens: int = None, tools: list = None):
    """
    Initialize chat model based on provider_type configuration.

    Supported provider types:
    - llamacpp: Uses ChatOpenAI with custom endpoint (llamacpp server with OpenAI-compatible API)
    - litellm: Uses ChatLiteLLM (gateway to multiple LLM providers)
    - openai: Uses ChatOpenAI (official OpenAI API or compatible services)
    """
    model_name = model_name or ENV_CONFIG["llm_model_name"]
    temperature = temperature if temperature is not None else ENV_CONFIG["llm_temperature"]
    max_tokens = max_tokens or ENV_CONFIG["llm_max_output_tokens"]

    provider_type = ENV_CONFIG["provider_type"]

    if provider_type == "llamacpp":
        # LlamaCpp server with OpenAI-compatible API
        model = ChatOpenAI(
            base_url=ENV_CONFIG["llamacpp_endpoint"],
            api_key=ENV_CONFIG["llamacpp_api_key"],
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )

    elif provider_type == "litellm":
        # LiteLLM gateway
        model = ChatLiteLLM(
            api_base=ENV_CONFIG["litellm_endpoint"],
            api_key=ENV_CONFIG["litellm_api_key"],
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )

    elif provider_type == "openai":
        # OpenAI or OpenAI-compatible service
        kwargs = {
            "model": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "api_key": ENV_CONFIG["openai_api_key"]
        }

        # Add custom endpoint if specified (for Azure OpenAI, etc.)
        if ENV_CONFIG.get("openai_endpoint"):
            kwargs["base_url"] = ENV_CONFIG["openai_endpoint"]

        model = ChatOpenAI(**kwargs)

    else:
        raise ValueError(f"Unsupported provider_type: {provider_type}. Must be 'llamacpp', 'litellm', or 'openai'")

    # Bind tools if provided
    if tools:
        model = model.bind_tools(tools)

    return model


LANGFUSE_HANDLER = _create_langfuse_handler()
