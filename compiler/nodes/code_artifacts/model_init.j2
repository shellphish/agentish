# ==========================================
# MODEL INITIALIZATION AND LANGFUSE SETUP
# ==========================================

def _load_env_config():
    """Load environment configuration for LLM and Langfuse from model_config.yaml."""
    import os
    return {
        "provider_type": os.getenv("PROVIDER_TYPE", "openai"),
        "llm_model_name": os.getenv("LLM_MODEL_NAME", "gpt-4"),
        "llm_temperature": float(os.getenv("LLM_TEMPERATURE", "0.0")),
        "llm_context_window": int(os.getenv("LLM_CONTEXT_WINDOW", "8192")),
        "llm_max_output_tokens": int(os.getenv("LLM_MAX_OUTPUT_TOKENS", "4096")),

        # Provider-specific settings
        "llamacpp_endpoint": os.getenv("LLAMACPP_ENDPOINT", ""),
        "llamacpp_api_key": os.getenv("LLAMACPP_API_KEY", ""),
        "litellm_endpoint": os.getenv("LITELLM_ENDPOINT", ""),
        "litellm_api_key": os.getenv("LITELLM_API_KEY", ""),
        "openai_api_key": os.getenv("OPENAI_API_KEY", ""),
        "openai_endpoint": os.getenv("OPENAI_ENDPOINT", ""),

        # Langfuse configuration
        "langfuse_public_key": os.getenv("LANGFUSE_PUBLIC_KEY", ""),
        "langfuse_secret_key": os.getenv("LANGFUSE_SECRET_KEY", ""),
        "langfuse_host": os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com"),
    }

ENV_CONFIG = _load_env_config()


def _create_langfuse_handler():
    """Create Langfuse callback handler if configured."""
    if not ENV_CONFIG.get("langfuse_public_key") or not ENV_CONFIG.get("langfuse_secret_key"):
        return None

    try:
        import os
        os.environ["LANGFUSE_PUBLIC_KEY"] = ENV_CONFIG["langfuse_public_key"]
        os.environ["LANGFUSE_SECRET_KEY"] = ENV_CONFIG["langfuse_secret_key"]
        os.environ["LANGFUSE_HOST"] = ENV_CONFIG["langfuse_host"]

        return CallbackHandler(
            public_key=ENV_CONFIG["langfuse_public_key"],
            secret_key=ENV_CONFIG["langfuse_secret_key"],
            host=ENV_CONFIG["langfuse_host"]
        )
    except Exception as e:
        print(f"Warning: Failed to initialize Langfuse: {e}")
        return None


def init_chat_model(model_name: str = None, temperature: float = None, max_tokens: int = None, tools: list = None):
    """
    Initialize chat model based on provider_type configuration.

    Supported provider types:
    - llamacpp: Uses ChatOpenAI with custom endpoint (llamacpp server with OpenAI-compatible API)
    - litellm: Uses ChatLiteLLM (gateway to multiple LLM providers)
    - openai: Uses ChatOpenAI (official OpenAI API or compatible services)
    """
    model_name = model_name or ENV_CONFIG["llm_model_name"]
    temperature = temperature if temperature is not None else ENV_CONFIG["llm_temperature"]
    max_tokens = max_tokens or ENV_CONFIG["llm_max_output_tokens"]

    provider_type = ENV_CONFIG["provider_type"]

    if provider_type == "llamacpp":
        # LlamaCpp server with OpenAI-compatible API
        model = ChatOpenAI(
            base_url=ENV_CONFIG["llamacpp_endpoint"],
            api_key=ENV_CONFIG["llamacpp_api_key"],
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )

    elif provider_type == "litellm":
        # LiteLLM gateway
        model = ChatLiteLLM(
            api_base=ENV_CONFIG["litellm_endpoint"],
            api_key=ENV_CONFIG["litellm_api_key"],
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )

    elif provider_type == "openai":
        # OpenAI or OpenAI-compatible service
        kwargs = {
            "model": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "api_key": ENV_CONFIG["openai_api_key"]
        }

        # Add custom endpoint if specified (for Azure OpenAI, etc.)
        if ENV_CONFIG.get("openai_endpoint"):
            kwargs["base_url"] = ENV_CONFIG["openai_endpoint"]

        model = ChatOpenAI(**kwargs)

    else:
        raise ValueError(f"Unsupported provider_type: {provider_type}. Must be 'llamacpp', 'litellm', or 'openai'")

    # Bind tools if provided
    if tools:
        model = model.bind_tools(tools)

    return model


LANGFUSE_HANDLER = _create_langfuse_handler()
