# Model runtime configuration used by both the ASL compiler and sandbox execution.
# Update the provider/model/keys to match the environment you want the generated
# LangGraph agent to run against.

# LLM Provider Configuration (REQUIRED)
provider: openai
model: "Qwen3-14B-Q8_0"
temperature: 0.0
recursion_limit: 500
use_tracing: true
context_window: 131072
max_output_tokens: 16384

# Provider Type Configuration (REQUIRED)
# Specifies which LLM backend infrastructure to use
# Options: "llamacpp", "litellm", "openai"
provider_type: "llamacpp"

# LlamaCpp Configuration
# Used when provider_type is "llamacpp"
# LlamaCpp hosts models with OpenAI-compatible API
llamacpp:
  endpoint: "http://128.111.49.59:4000/v1"
  api_key: "local_llm_is_da_best"

# LiteLLM Configuration
# Used when provider_type is "litellm"
# LiteLLM acts as a gateway to access different LLMs
litellm:
  endpoint: "http://192.35.222.19:4000"
  api_key: "local_llm_is_da_best"

# OpenAI Configuration (Optional)
# Used when provider_type is "openai"
# Can specify custom endpoint for Azure OpenAI, etc.
openai:
  api_key: ""  # OpenAI API key
  endpoint: ""  # Optional: custom endpoint (e.g., Azure OpenAI)

# Langfuse Configuration (REQUIRED)
# Langfuse provides observability and tracing for LLM calls
langfuse:
  host: "http://192.35.222.19.nip.io:3000"
  public_key: "pk-lf-a0ff2b74-a0c8-4ce1-90c1-cf90771c95f4"
  secret_key: "sk-lf-a2ee69ee-f7d5-4a42-a8d5-bfbdb37a517b"

enable_tracing: true